{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Poetry Notebook\n",
        "\n",
        "In this notebook we will be implementing GPT to generate text based on the work of Edgar Allan Poe."
      ],
      "metadata": {
        "id": "cqUrnIpoqAHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing dependencies\n",
        "!pip install tiktoken\n",
        "!pip install torch\n",
        "\n",
        "# Downloading dataset from the GitHub\n",
        "!wget https://raw.githubusercontent.com/kocenko/Poetry-Synthesis/dev/data/poe_data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d5dI4LKXBp",
        "outputId": "8e4313f2-d8b1-4003-941e-69d04bb260d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "--2023-05-16 19:52:05--  https://raw.githubusercontent.com/kocenko/Poetry-Synthesis/dev/data/poe_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1930488 (1.8M) [text/plain]\n",
            "Saving to: ‘poe_data.txt’\n",
            "\n",
            "poe_data.txt        100%[===================>]   1.84M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-16 19:52:05 (47.9 MB/s) - ‘poe_data.txt’ saved [1930488/1930488]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "PqtdnLBxN3sQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "zLSR7xdlPQaN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class definition\n",
        "### (Option) We can use different data to train it on\n",
        "### (Option) What if the context affects not the following\n",
        "###          but the one after the following token? (bigger offset)\n",
        "\n",
        "class PoeDataset(Dataset):\n",
        "    valid_split_params = [\"train\", \"valid\"]\n",
        "\n",
        "    def __init__(self, text: str, split: str, split_ratio: float, context_length: int, tokenizer, offset: int = 1):\n",
        "        ''' Poe Dataset constructor\n",
        "\n",
        "        Args:\n",
        "            str:\n",
        "                file_path: Path to the file containing dataset\n",
        "                splt: String indicating what type of data this dataset contains\n",
        "            float:\n",
        "                split_ratio: Value between (0, 1] of what should be the ratio\n",
        "                             between training and validation set\n",
        "            int:\n",
        "                context_length: Length of the context\n",
        "                offset: An offset between the end of the context and the target\n",
        "        '''\n",
        "\n",
        "        assert split in PoeDataset.valid_split_params, f\"{split} is the wrong split type\"\n",
        "        assert split_ratio <= 1 and split_ratio > 0, f\"Split ratio value should be from range (0, 1]\"\n",
        "        assert len(text) > 0, f\"Dataset file should not be empty\"\n",
        "        assert context_length < len(text), f\"Context length should not be more than {len(text) - 1}\"\n",
        "\n",
        "        self.text = text\n",
        "        self.offset = offset\n",
        "        self.context_length = context_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = torch.tensor(self.tokenizer.encode(self.text), dtype=torch.int32, device=device)\n",
        "\n",
        "        split_idx = int(len(self.data) * split_ratio)\n",
        "        if split == \"train\":\n",
        "            self.data = self.data[:split_idx]\n",
        "        else:\n",
        "            self.data = self.data[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        ''' Returns the size of the dataset\n",
        "        \n",
        "        Returns:\n",
        "            Number of possible shifts in the dataset for choosing the context chunk\n",
        "        '''\n",
        "        return len(self.data) - self.context_length - self.context_length + 1\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ''' Returns an item of given index\n",
        "\n",
        "        Params:\n",
        "            index: Which item should be returned\n",
        "        \n",
        "        Returns:\n",
        "            Sample of given index\n",
        "        '''\n",
        "        \n",
        "        x = self.data[index: index + self.context_length]\n",
        "        y = self.data[index + self.offset: index + self.context_length + self.offset]\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "4K9Stu5Uy3Ym"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined tokenizer class\n",
        "import torch\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    ''' Class for character-wise tokenization'''\n",
        "\n",
        "    def __init__(self, text: str):\n",
        "        assert len(text) > 0, \"Text used for creating tokenizer cannot be empty\"\n",
        "\n",
        "        self.text = text\n",
        "        self.symbols = sorted(list(set(self.text)))\n",
        "        self.vocab_size = len(self.symbols)\n",
        "        self.stoi = { ch:i for i, ch in enumerate(self.symbols)}\n",
        "        self.itos = { i:ch for i, ch in enumerate(self.symbols)}\n",
        "\n",
        "    def encode(self, text: str) -> List:\n",
        "        ''' Encodes string to list of ints '''\n",
        "\n",
        "        return [self.stoi[ch] for ch in text]\n",
        "    \n",
        "    def decode(self, tokens: List) -> str:\n",
        "        ''' Decodes list of ints to string '''\n",
        "        \n",
        "        return ''.join([self.itos[token] for token in tokens])\n"
      ],
      "metadata": {
        "id": "fYZho5RId81o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Decoder Class definition\n",
        "### (Option) Different split, test data?\n",
        "from typing import Tuple\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class OnlyDecoder(nn.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.n_embed = config[\"n_embed\"]\n",
        "        self.context_length = config[\"context_length\"]\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.n_embed, device=device)\n",
        "        self.pos_embedding_table = nn.Embedding(self.context_length, self.n_embed, device=device)\n",
        "        self.lin = nn.Linear(self.n_embed, self.vocab_size, device=device)\n",
        "\n",
        "    def forward(self, token_idx: int, targets=None):\n",
        "        B, T = token_idx.shape\n",
        "        token_embedding = self.token_embedding_table(token_idx)\n",
        "        pos_embedding = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = token_embedding + pos_embedding\n",
        "        logits = self.lin(x)\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          targets = targets.type(torch.LongTensor).to(device)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate_new_text(self, idx, sym_limit: int) -> torch.Tensor:\n",
        "        for _ in range(sym_limit):\n",
        "          idx = idx if idx.size(1) <= self.context_length else idx[:, -self.context_length:]\n",
        "          logits, loss = self(idx)\n",
        "          logits = logits[:, -1, :]\n",
        "          probabilities = F.softmax(logits, dim=-1)\n",
        "          idx_next = torch.multinomial(probabilities, num_samples=1) # Take best\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "x5Isk9hXQKtQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_loss(model, iterations, batch_size, train_set, val_set):\n",
        "    ''' Used to evalute model by averaging on many iterations\n",
        "    \n",
        "    Args:\n",
        "        model: Evaluated model\n",
        "        iterations: Number of iterations to average through\n",
        "        batch_size: Batch size\n",
        "        train_set: Training dataset\n",
        "        val_set: Validation dataset\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with averaged losses for 'train' nad 'valid'\n",
        "    '''\n",
        "\n",
        "    split_type = [\"train\", \"valid\"]\n",
        "    outcome_losses = {}\n",
        "    model.eval()\n",
        "    for t, split in enumerate([train_set, val_set]):\n",
        "        loader = DataLoader(split, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "        loader = iter(loader)\n",
        "        losses = torch.zeros(iterations)\n",
        "        for i in range(iterations):\n",
        "            x, y = loader.__next__()\n",
        "            _, loss = model(x, y)\n",
        "            losses[i] = loss.item()\n",
        "        outcome_losses[split_type[t]] = losses.mean()\n",
        "    model.train()\n",
        "    return outcome_losses\n",
        "\n",
        "\n",
        "def train_model(model, train_set, valid_set, hyper_params: dict, device):\n",
        "    ''' Trains the model\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_set: Training dataset\n",
        "        valid_set: Validation dataset\n",
        "        hyper_params: dict of hyperparameters\n",
        "    '''\n",
        "\n",
        "    lr = hyper_params[\"lr\"]\n",
        "    epochs = hyper_params[\"epochs\"]\n",
        "    batch_size = hyper_params[\"batch_size\"]\n",
        "    eval_each = hyper_params[\"eval_each\"]\n",
        "    eval_iterations = hyper_params[\"eval_iterations\"]\n",
        "    break_iter = hyper_params[\"break_iter\"]\n",
        "\n",
        "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        for i, (x, y) in enumerate(train_dataloader):\n",
        "            if i == break_iter:\n",
        "                break\n",
        "\n",
        "            if i % eval_each == 0:\n",
        "                losses = calc_loss(model, eval_iterations, batch_size, train_set, valid_set)\n",
        "                print(f\"Epoch: {e} Step: {i}, train loss: {losses['train']:.4f}, val loss: {losses['valid']:.4f}\")\n",
        "            \n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, loss = model(x, y)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "Wum581WusvrN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the dataset parameters\n",
        "### (Option 1) We can use different tokenizer, like SentencePiece\n",
        "### (Option 2) We can build our own tokenizer, using huggingface library\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"poe_data.txt\"\n",
        "\n",
        "# Reading file, preparing tokenizer\n",
        "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "# Setting up dataset\n",
        "split_ratio = 0.85\n",
        "context_length = 8\n",
        "offset = 1  # I am wondering what would be the results for 2, for example\n",
        "custom_tokenizer = False\n",
        "\n",
        "if custom_tokenizer:\n",
        "    tokenizer = Tokenizer(text)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "else:\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    vocab_size = tokenizer.n_vocab\n",
        "\n",
        "# Setting up model\n",
        "net_config = { \"vocab_size\": vocab_size,\n",
        "               \"n_embed\": 32,\n",
        "               \"context_length\": context_length}\n",
        "symbols_limit = 50\n",
        "model = OnlyDecoder(net_config)\n",
        "model.to(device)\n",
        "\n",
        "# Training parameters\n",
        "hypers = {\n",
        "    \"lr\": .3e-4,\n",
        "    \"epochs\": 1,\n",
        "    \"batch_size\": 32,\n",
        "    \"eval_each\": 200,\n",
        "    \"eval_iterations\": 200,\n",
        "    \"break_iter\": 1000\n",
        "}\n",
        "\n",
        "# Training\n",
        "train_set = PoeDataset(text, 'train', split_ratio, context_length, tokenizer, offset=offset)\n",
        "val_set = PoeDataset(text, 'valid', split_ratio, context_length, tokenizer, offset=offset)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=4, shuffle=True, drop_last=True)\n",
        "train_model(model, train_set, val_set, hypers, device=device)\n",
        "\n",
        "# Test it\n",
        "starter = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(tokenizer.decode(model.generate_new_text(starter, 20)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1nwQWyQqNCu",
        "outputId": "942467fc-3ac3-4ca6-c8e0-3ea38f6743c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Step: 0, train loss: 11.8450, val loss: 11.8448\n",
            "Epoch: 0 Step: 200, train loss: 11.7575, val loss: 11.7675\n",
            "Epoch: 0 Step: 400, train loss: 11.6738, val loss: 11.6752\n",
            "Epoch: 0 Step: 600, train loss: 11.5893, val loss: 11.5945\n",
            "Epoch: 0 Step: 800, train loss: 11.5153, val loss: 11.5142\n",
            " fd.appcompat.getCount complementaryør Adoption blade valuation<State\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGrCGZ2W1c6g"
      },
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}