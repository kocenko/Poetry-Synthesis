{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Poetry Notebook\n",
        "\n",
        "In this notebook we will be implementing GPT to generate text based on the work of Edgar Allan Poe."
      ],
      "metadata": {
        "id": "cqUrnIpoqAHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing dependencies\n",
        "!pip install torch\n",
        "\n",
        "# Downloading dataset from the GitHub\n",
        "!wget https://raw.githubusercontent.com/kocenko/Poetry-Synthesis/dev/data/poe_data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d5dI4LKXBp",
        "outputId": "453f8bc0-5184-41d4-966f-aea9f4b20efb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "--2023-05-16 16:49:03--  https://raw.githubusercontent.com/kocenko/Poetry-Synthesis/dev/data/poe_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1930488 (1.8M) [text/plain]\n",
            "Saving to: ‘poe_data.txt’\n",
            "\n",
            "poe_data.txt        100%[===================>]   1.84M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-05-16 16:49:04 (252 MB/s) - ‘poe_data.txt’ saved [1930488/1930488]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "PqtdnLBxN3sQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\""
      ],
      "metadata": {
        "id": "zLSR7xdlPQaN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class definition\n",
        "### (Option) We can use different data to train it on\n",
        "### (Option) What if the context affects not the following\n",
        "###          but the one after the following token? (bigger offset)\n",
        "\n",
        "class PoeDataset(Dataset):\n",
        "    valid_split_params = [\"train\", \"valid\"]\n",
        "\n",
        "    def __init__(self, text: str, split: str, split_ratio: float, context_length: int, tokenizer, offset: int = 1):\n",
        "        ''' Poe Dataset constructor\n",
        "\n",
        "        Args:\n",
        "            str:\n",
        "                file_path: Path to the file containing dataset\n",
        "                splt: String indicating what type of data this dataset contains\n",
        "            float:\n",
        "                split_ratio: Value between (0, 1] of what should be the ratio\n",
        "                             between training and validation set\n",
        "            int:\n",
        "                context_length: Length of the context\n",
        "                offset: An offset between the end of the context and the target\n",
        "        '''\n",
        "\n",
        "        assert split in PoeDataset.valid_split_params, f\"{split} is the wrong split type\"\n",
        "        assert split_ratio <= 1 and split_ratio > 0, f\"Split ratio value should be from range (0, 1]\"\n",
        "        assert len(text) > 0, f\"Dataset file should not be empty\"\n",
        "        assert context_length < len(text), f\"Context length should not be more than {len(text) - 1}\"\n",
        "\n",
        "        self.text = text\n",
        "        self.offset = offset\n",
        "        self.context_length = context_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = torch.tensor(self.tokenizer.encode(self.text), dtype=torch.int32, device=device)\n",
        "\n",
        "        split_idx = int(len(self.data) * split_ratio)\n",
        "        if split == \"train\":\n",
        "            self.data = self.data[:split_idx]\n",
        "        else:\n",
        "            self.data = self.data[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        ''' Returns the size of the dataset\n",
        "        \n",
        "        Returns:\n",
        "            Number of possible shifts in the dataset for choosing the context chunk\n",
        "        '''\n",
        "        return len(self.data) - self.context_length - self.context_length + 1\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ''' Returns an item of given index\n",
        "\n",
        "        Params:\n",
        "            index: Which item should be returned\n",
        "        \n",
        "        Returns:\n",
        "            Sample of given index\n",
        "        '''\n",
        "        assert index > 0 and index < self.__len__()\n",
        "\n",
        "        x = self.data[index: index + self.context_length]\n",
        "        y = self.data[index + self.offset: index + self.context_length + self.offset]\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "4K9Stu5Uy3Ym"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined tokenizer class\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    ''' Class for character-wise tokenization'''\n",
        "\n",
        "    def __init__(self, text: str):\n",
        "        assert len(text) > 0, \"Text used for creating tokenizer cannot be empty\"\n",
        "\n",
        "        self.text = text\n",
        "        self.symbols = sorted(list(set(self.text)))\n",
        "        self.vocab_size = len(self.symbols)\n",
        "        self.stoi = { ch:i for i, ch in enumerate(self.symbols)}\n",
        "        self.itos = { i:ch for i, ch in enumerate(self.symbols)}\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        ''' Encodes string to list of ints '''\n",
        "\n",
        "        return [self.stoi[ch] for ch in text]\n",
        "    \n",
        "    def decode(self, tokens: List[int]) -> str:\n",
        "        ''' Decodes list of ints to string '''\n",
        "        \n",
        "        return ''.join([self.itos[token] for token in tokens])\n"
      ],
      "metadata": {
        "id": "fYZho5RId81o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the dataset parameters\n",
        "### (Option 1) We can use different tokenizer, like SentencePiece\n",
        "### (Option 2) We can build our own tokenizer, using huggingface library\n",
        "\n",
        "epochs = 1  # Just for now\n",
        "split_ratio = 0.85\n",
        "context_length = 8\n",
        "offset = 1  # I am wondering what would be the results for 2, for example\n",
        "batch_size = 4\n",
        "file_path = \"poe_data.txt\"\n",
        "\n",
        "# Reading file, preparing tokenizer\n",
        "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "tokenizer = Tokenizer(text)\n",
        "net_config = {\n",
        "    \"vocab_size\": lambda: tokenizer.vocab_size\n",
        "}\n",
        "\n",
        "# Dataset and dataloader\n",
        "train_set = PoeDataset(text, 'train', split_ratio, context_length, tokenizer, offset=offset)\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "x, y = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "irucbIntzcQ3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Decoder Class definition\n",
        "### (Option) Different split, test data?\n",
        "\n",
        "\n",
        "class OnlyDecoder(nn.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.embedding_table = nn.Embedding(self.vocab_size, self.vocab_size, device=device)\n",
        "\n",
        "    def forward(self, token_idx: int, targets: torch.tensor) -> torch.tensor:\n",
        "        logits = self.embedding_table(token_idx)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "x5Isk9hXQKtQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up net parameters\n",
        "for key, value in net_config.items():\n",
        "  if callable(value):\n",
        "    net_config[key] = value()\n",
        "  else:\n",
        "    net_config[key]\n",
        "\n",
        "# Testing Model\n",
        "model = OnlyDecoder(net_config)\n",
        "out = model(x, y)"
      ],
      "metadata": {
        "id": "-p-fJC2FWpSy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1ADCsPMaq4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}