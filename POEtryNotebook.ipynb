{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Poetry Notebook\n",
        "\n",
        "In this notebook we will be implementing GPT to generate text based on the work of Edgar Allan Poe."
      ],
      "metadata": {
        "id": "cqUrnIpoqAHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing dependencies\n",
        "!pip install torch\n",
        "\n",
        "# Downloading dataset from the GitHub\n",
        "!wget https://raw.githubusercontent.com/kocenko/Poetry-Synthesis/dev/data/poe_data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d5dI4LKXBp",
        "outputId": "86ff05d6-3eb7-48d4-9e7f-249f7b0342a5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "--2023-05-16 17:51:23--  https://raw.githubusercontent.com/kocenko/Poetry-Synthesis/dev/data/poe_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1930488 (1.8M) [text/plain]\n",
            "Saving to: ‘poe_data.txt.1’\n",
            "\n",
            "poe_data.txt.1      100%[===================>]   1.84M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-05-16 17:51:23 (249 MB/s) - ‘poe_data.txt.1’ saved [1930488/1930488]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "PqtdnLBxN3sQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\""
      ],
      "metadata": {
        "id": "zLSR7xdlPQaN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class definition\n",
        "### (Option) We can use different data to train it on\n",
        "### (Option) What if the context affects not the following\n",
        "###          but the one after the following token? (bigger offset)\n",
        "\n",
        "class PoeDataset(Dataset):\n",
        "    valid_split_params = [\"train\", \"valid\"]\n",
        "\n",
        "    def __init__(self, text: str, split: str, split_ratio: float, context_length: int, tokenizer, offset: int = 1):\n",
        "        ''' Poe Dataset constructor\n",
        "\n",
        "        Args:\n",
        "            str:\n",
        "                file_path: Path to the file containing dataset\n",
        "                splt: String indicating what type of data this dataset contains\n",
        "            float:\n",
        "                split_ratio: Value between (0, 1] of what should be the ratio\n",
        "                             between training and validation set\n",
        "            int:\n",
        "                context_length: Length of the context\n",
        "                offset: An offset between the end of the context and the target\n",
        "        '''\n",
        "\n",
        "        assert split in PoeDataset.valid_split_params, f\"{split} is the wrong split type\"\n",
        "        assert split_ratio <= 1 and split_ratio > 0, f\"Split ratio value should be from range (0, 1]\"\n",
        "        assert len(text) > 0, f\"Dataset file should not be empty\"\n",
        "        assert context_length < len(text), f\"Context length should not be more than {len(text) - 1}\"\n",
        "\n",
        "        self.text = text\n",
        "        self.offset = offset\n",
        "        self.context_length = context_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = torch.tensor(self.tokenizer.encode(self.text), dtype=torch.int32, device=device)\n",
        "\n",
        "        split_idx = int(len(self.data) * split_ratio)\n",
        "        if split == \"train\":\n",
        "            self.data = self.data[:split_idx]\n",
        "        else:\n",
        "            self.data = self.data[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        ''' Returns the size of the dataset\n",
        "        \n",
        "        Returns:\n",
        "            Number of possible shifts in the dataset for choosing the context chunk\n",
        "        '''\n",
        "        return len(self.data) - self.context_length - self.context_length + 1\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ''' Returns an item of given index\n",
        "\n",
        "        Params:\n",
        "            index: Which item should be returned\n",
        "        \n",
        "        Returns:\n",
        "            Sample of given index\n",
        "        '''\n",
        "        \n",
        "        x = self.data[index: index + self.context_length]\n",
        "        y = self.data[index + self.offset: index + self.context_length + self.offset]\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "4K9Stu5Uy3Ym"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined tokenizer class\n",
        "import torch\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    ''' Class for character-wise tokenization'''\n",
        "\n",
        "    def __init__(self, text: str):\n",
        "        assert len(text) > 0, \"Text used for creating tokenizer cannot be empty\"\n",
        "\n",
        "        self.text = text\n",
        "        self.symbols = sorted(list(set(self.text)))\n",
        "        self.vocab_size = len(self.symbols)\n",
        "        self.stoi = { ch:i for i, ch in enumerate(self.symbols)}\n",
        "        self.itos = { i:ch for i, ch in enumerate(self.symbols)}\n",
        "\n",
        "    def encode(self, text: str) -> List:\n",
        "        ''' Encodes string to list of ints '''\n",
        "\n",
        "        return [self.stoi[ch] for ch in text]\n",
        "    \n",
        "    def decode(self, tokens: List) -> str:\n",
        "        ''' Decodes list of ints to string '''\n",
        "        \n",
        "        return ''.join([self.itos[token] for token in tokens])\n"
      ],
      "metadata": {
        "id": "fYZho5RId81o"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Decoder Class definition\n",
        "### (Option) Different split, test data?\n",
        "from typing import Tuple\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class OnlyDecoder(nn.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.embedding_table = nn.Embedding(self.vocab_size, self.vocab_size, device=device)\n",
        "\n",
        "    def forward(self, token_idx: int, targets=None):\n",
        "        logits = self.embedding_table(token_idx)\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          targets = targets.type(torch.LongTensor).to(device)\n",
        "\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate_new_text(self, idx, sym_limit: int) -> torch.Tensor:\n",
        "        for _ in range(sym_limit):\n",
        "          logits, loss = self(idx)\n",
        "          logits = logits[:, -1, :]\n",
        "          probabilities = F.softmax(logits, dim=-1)\n",
        "          idx_next = torch.multinomial(probabilities, num_samples=1) # Take best\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "x5Isk9hXQKtQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_loss(model, iterations, batch_size, train_set, val_set):\n",
        "    ''' Used to evalute model by averaging on many iterations\n",
        "    \n",
        "    Args:\n",
        "        model: Evaluated model\n",
        "        iterations: Number of iterations to average through\n",
        "        batch_size: Batch size\n",
        "        train_set: Training dataset\n",
        "        val_set: Validation dataset\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with averaged losses for 'train' nad 'valid'\n",
        "    '''\n",
        "\n",
        "    split_type = [\"train\", \"valid\"]\n",
        "    outcome_losses = {}\n",
        "    model.eval()\n",
        "    for t, split in enumerate([train_set, val_set]):\n",
        "        loader = DataLoader(split, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "        loader = iter(loader)\n",
        "        losses = torch.zeros(iterations)\n",
        "        for i in range(iterations):\n",
        "            x, y = loader.__next__()\n",
        "            _, loss = model(x, y)\n",
        "            losses[i] = loss.item()\n",
        "        outcome_losses[split_type[t]] = losses.mean()\n",
        "    model.train()\n",
        "    return outcome_losses\n",
        "\n",
        "\n",
        "def train_model(model, train_set, valid_set, hyper_params: dict):\n",
        "    ''' Trains the model\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_set: Training dataset\n",
        "        valid_set: Validation dataset\n",
        "        hyper_params: dict of hyperparameters\n",
        "    '''\n",
        "\n",
        "    lr = hyper_params[\"lr\"]\n",
        "    epochs = hyper_params[\"epochs\"]\n",
        "    batch_size = hyper_params[\"batch_size\"]\n",
        "    eval_each = hyper_params[\"eval_each\"]\n",
        "    eval_iterations = hyper_params[\"eval_iterations\"]\n",
        "    break_iter = hyper_params[\"break_iter\"]\n",
        "\n",
        "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        for i, (x, y) in enumerate(train_dataloader):\n",
        "            if i == break_iter:\n",
        "                break\n",
        "\n",
        "            if i % eval_each == 0:\n",
        "                losses = calc_loss(model, eval_iterations, batch_size, train_set, valid_set)\n",
        "                print(f\"Epoch: {e} Step: {i}, train loss: {losses['train']:.4f}, val loss: {losses['valid']:.4f}\")\n",
        "            \n",
        "            logits, loss = model(x, y)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "Wum581WusvrN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the dataset parameters\n",
        "### (Option 1) We can use different tokenizer, like SentencePiece\n",
        "### (Option 2) We can build our own tokenizer, using huggingface library\n",
        "\n",
        "file_path = \"poe_data.txt\"\n",
        "\n",
        "# Reading file, preparing tokenizer\n",
        "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "# Setting up dataset\n",
        "split_ratio = 0.85\n",
        "context_length = 8\n",
        "offset = 1  # I am wondering what would be the results for 2, for example\n",
        "\n",
        "tokenizer = Tokenizer(text)\n",
        "\n",
        "# Setting up model\n",
        "net_config = {\n",
        "    \"vocab_size\": tokenizer.vocab_size\n",
        "}\n",
        "symbols_limit = 50\n",
        "model = OnlyDecoder(net_config)\n",
        "\n",
        "# Training parameters\n",
        "hypers = {\n",
        "    \"lr\": .3e-4,\n",
        "    \"epochs\": 1,\n",
        "    \"batch_size\": 32,\n",
        "    \"eval_each\": 200,\n",
        "    \"eval_iterations\": 200,\n",
        "    \"break_iter\": 1000\n",
        "}\n",
        "\n",
        "# Training\n",
        "train_set = PoeDataset(text, 'train', split_ratio, context_length, tokenizer, offset=offset)\n",
        "val_set = PoeDataset(text, 'valid', split_ratio, context_length, tokenizer, offset=offset)\n",
        "train_model(model, train_set, val_set, hypers)\n",
        "\n",
        "# Test it\n",
        "starter = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(tokenizer.decode(model.generate_new_text(starter, 20)[0].tolist()))"
      ],
      "metadata": {
        "id": "i1nwQWyQqNCu",
        "outputId": "8ac14a81-e836-4ad7-bae7-cf761602d0d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Step: 0, train loss: 5.1475, val loss: 5.1490\n",
            "Epoch: 0 Step: 200, train loss: 5.1323, val loss: 5.1439\n",
            "Epoch: 0 Step: 400, train loss: 5.1243, val loss: 5.1281\n",
            "Epoch: 0 Step: 600, train loss: 5.1216, val loss: 5.1188\n",
            "Epoch: 0 Step: 800, train loss: 5.1122, val loss: 5.1111\n",
            "\n",
            "“;äzLç[?5g‘—twfhv2 t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGrCGZ2W1c6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}